<!DOCTYPE html>
<html lang="en" class="no-js">
	<head>
		<meta charset="UTF-8" />
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
		<meta name="viewport" content="width=device-width, initial-scale=1.0"> 
		<title>UCSB Physics129AL Introduction to Scientific Computing</title>
		<meta name="description" content="Physics 129AL" />
		<meta name="keywords" content="physics" />
		
		<link rel="shortcut icon" href=" vscode-file://vscode-app/snap/code/137/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.htmlfavicon.ico">
		<link rel="stylesheet" type="text/css" href=" static/css/normalize.css" />
		<link rel="stylesheet" type="text/css" href=" static/css/demo.css" />
		<link rel="stylesheet" type="text/css" href=" static/css/component.css" />
		<script src="  static/js/modernizr.custom.js"></script>
		<link href="  static/css/singlePageTemplate.css" rel="stylesheet" type="text/css">
<link href="  static/css/multiColumnTemplate.css" rel="stylesheet" type="text/css">
<link href=" static/css/layout1.css" rel="stylesheet" type="text/css">
	<script src=" static/js/mainpage.js"></script>
	<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      }
    });
  </script>
	</head>
	<body>
<!-- ------------------------------------------------------------------------------------------------				 -->



<div class="container">
	<ul id="gn-menu" class="gn-menu-main">
	  <li class="gn-trigger">
		  <a class="gn-icon gn-icon-menu"><span>Menu</span></a>
		  <nav class="gn-menu-wrapper">
			  
			  <div class="gn-scroller">
				  
				  <ul class="gn-menu">
					  

					  <li><a class="gn-icon gn-icon-cog", href="announcements.html">Announcements</a></li>
					   <div class="dropdown">
						  <div class="dropdown-button"><li><a class="gn-icon gn-icon-cog", href="courseinformation.html">Course Information</a></li>
						  </div>
						  <div class="dropdown-content">
							  <a href="courseinformation_getstart.html">Get Started</a>
							  <a href="common_issue.html">Common Issue</a>

						  </div>
					  </div>

					  
					  <div class="dropdown">
						<div class="dropdown-button"> <li><a class="gn-icon gn-icon-article">Course Material</a></li>
						</div>
						<div class="dropdown-content">
							<a href="unix.html">UNIX Commands and Bash Scripts</a>
							<a href="git.html"> Version Control Using Git </a>
							<a href="stats_1.html"> Statistical Processes I </a>
							<a href="stats_2.html"> Statistical Processes II </a>

						</div>
					</div>
						 
						  <!-- <li>
							  <a class="gn-icon gn-icon-videos">Problem Sets</a>
							  </li> -->
						   <!-- <li><a class="gn-icon gn-icon-pictures"> Statistics  </a></li> -->
							  <!-- <li><a class="gn-icon gn-icon-videos">Resources</a></li> -->
					  
							  <li>
								  <a class="gn-icon gn-icon-archive", href="problem_set.html">Problem Sets</a>
								  </li>
					  <li>
						  <a class="gn-icon gn-icon-download", href="resourse.html">  Resources</a>
						   <!-- About Section 
						  <ul class="gn-submenu">
							  <li><a class="gn-icon gn-icon-illustrator">Vector Illustrations</a></li>
							  <li><a class="gn-icon gn-icon-photoshop">Photoshop files</a></li>
							  
						  </ul>
			  -->
					  </li>

						  <!-- <li><a class="gn-icon gn-icon-cog">Statistics</a></li> -->
					  
				  </ul>
			  </div><!-- /gn-scroller -->
		  </nav>
	  </li>
	  <li><a href="index.html"><span style="color: rgb(255, 255, 255);">Home</span></a></li>
		  <li><a href="https://www.physics.ucsb.edu/"  target="_blank"><span style="color: rgb(255, 255, 255);">Department of Physics â€¢ UC Santa Barbara</span></a></li>
		  <!--  -->
		  
	  </ul>

  </div><!-- /container -->
  <script src=" static/js/classie.js"></script>
  <script src=" static/js/gnmenu.js"></script>
  <script>
	  new gnMenu( document.getElementById( 'gn-menu' ) );
  </script>
  
<!-- ------------------------------------------------------------------------------------------------				 -->
<header>

  </header>
  
		<div class="img-container2">
			
		<!-- <img src="  ../static/figures/Jupternew.png" class="transparent-image" class="fluidimg" width="1600" height="100" /> -->
		<!-- <div class="box1">
            <h2>Container 1</h2>
            <p>This is some text inside Container 1.</p>

		
		</div> -->
		<!-- <div class="top-left">
		    <h1 style="color: aliceblue"> Testing</h1>
		  </div> 
	<div class="top-right">
	  <h1 style="color: aliceblue"> Testing</h1>
	</div>  -->
	<div class="center-post-git">
		<h1 style="color: aliceblue", class="large-h1"> Statistical Processes in Physics I </h1>
	  </div> 
	  <p style="color: rgb(255, 255, 255); text-align: center;">A PDF version can be founded in here,</p> 
	  <a class="a-val" href="static/docs/Physics129_stat_1.pdf" target="_blank",style="color: aliceblue">Download PDF</a>

		  <div class="sec3-container">
		<article class="footer_column">

			<h3 style="color: aliceblue", class="header-h1"> Bernoulli process </h3>

			<p style="color: rgb(255, 255, 255); line-height: 1.5; ">A single trial in a Bernoulli process is called a Bernoulli trial, which has two outcomes: success, denoted as 1, with probability $p$, and failure, denoted as 0, with probability $1-p$.
				Tossing a coin is a simple example of a Bernoulli process since each toss of the coin can result in one of two possible outcomes: "Heads" (1) or "Tails" (0) with a probability mass function of $p$ and $1-p$,  	 </p>
				<div class="math">$$         P_\delta (p,X) =
					\begin{cases}
					   p, &  X=1 \\
						1-p, & X=0
					\end{cases}.  $$</div>
					<p style="color: rgb(255, 255, 255); line-height: 1.5; "> In quantum mechanics, the Stern-Gerlach experiment serves as a classical illustration of a Bernoulli process. 
					An electron passes through an inhomogeneous magnetic field, yielding two distinct outcomes: 
					the particles are deflected either "up" or "down", based on their intrinsic angular momentum, known as spin. 
					In particular, the initial spin quantum state of a single electron is given by,</p>
					<div class="math">$$       |\Psi \rangle_e= \sqrt{p} |\uparrow\rangle+e^{i\phi} \sqrt{1-p} |\downarrow \rangle,  $$</div>
					<p style="color: rgb(255, 255, 255); line-height: 1.5; ">and $\phi$ is a general phase factor. 
						The probability is then given by the overlap, 
						e.g. $|\langle \uparrow|\Psi \rangle _e|^2=p$ and $| \langle \downarrow|\Psi \rangle _e|^2=1-p$. 
						We emphasize that the trials in a Bernoulli process are independent.</p>
				
						<p style="color: rgb(255, 255, 255);">  </p>
				<h3 style="color: aliceblue", class="header-h1"> Binomial process </h3>
				<p style="color: rgb(255, 255, 255); line-height: 1.5; ">A binomial process consists of a sequence of $N$ Bernoulli trials, 
					and the probability mass function for having $M$ successes is determined by the binomial distribution,
					</p>
					<div class="math">$$     P_B(p,M)=\binom{N}{M} p^M (1-p)^{N-M}.  $$</div>
					<p  style="color: rgb(255, 255, 255); line-height: 1.5; ">  Instead of having a single electron in the Stern-Gerlach experiment, a beam (or a pack) of electrons (with fixed number $N$) is utilized, 
					and the outcome intensity (snapshot) is characterized by the binomial distribution. The many-body quantum state can be written as,	</p>
					<div class="math">$$       |\Psi\rangle_{\rm e,beam} =  \bigotimes^{N}_j \left(\sqrt{p}|\uparrow_j \rangle+e^{i\phi_j} \sqrt{1-p} |\downarrow_j \rangle \right),  $$</div>
					<p style="color: rgb(255, 255, 255); line-height: 1.5; ">and the probability of observing a particular N-body quantum state, e.g. $|\Psi \rangle_{0}=|\uparrow \uparrow \downarrow \uparrow \dots \downarrow \rangle$, 
						with $M$ spin up electrons is given by the binomial distribution.  </p>
						<p style="color: rgb(255, 255, 255); line-height: 1.5; ">The key difference is that a Bernoulli process involves independent trials with two outcomes, 
							while a binomial process counts successes in a fixed number of these trials. 
						</p>
						<h3 style="color: aliceblue", class="header-h1"> Poisson process </h3>
						<p style="color: rgb(255, 255, 255); line-height: 1.5; ">The Poisson process assumes that events are rare, independent, and isolated*. 
						  The probability of $M$ events occurring in a time interval $T$ is described by the probability mass function 
						  $P(M,T) = (\gamma T)^M/M! e^{-\gamma T}$, where $\gamma$ represents a general rate function. 
						  The Poisson distribution can be derived from the binomial distribution in the limit $dt \to 0$,   </p>
						  <div class="math">$$  P(M,T)=\lim_{dt \to 0}\frac{N!}{M!(N-M)!}(\gamma dt)^M (1-\gamma dt)^{N-M} =\frac{(\gamma T)^M}{M!} e^{-\gamma T},  $$</div>
						  <p style="color: rgb(255, 255, 255); line-height: 1.5; "> where $N=T/dt$, and the limits are, </p>
						  <div class="math">$$      \lim_{dt \to 0} \frac{N!}{M!(N-M)!}(\gamma dt)^M =\frac{(\gamma N dt)^M}{M!}=\frac{(\gamma T)^M}{M!}, \hspace{0.4cm}   \lim_{dt \to 0} (1-\gamma dt)^{N-M} =e^{-\gamma T}.   $$</div>
						  <p style="color: rgb(255, 255, 255); line-height: 1.5; "> The Poisson distribution is particularly useful in various applications, including astronomy, quantum optics, and telecommunications, 
							  where the detection of individual photons/particles is essential, 
							  and their arrival can be modeled as a Poisson process. 
							  For example, Poisson process is used to study atomic decay, 
							  photon detection, and quantum tunneling. 
							  Let's recall the Stern-Gerlach experiment of a beam of electrons. 
							  When $p \to 0$, the binomial distribution becomes the Poisson distribution. 
							  We should note that the number of total events goes to infinity.  </p>

							  <p style="color: rgb(255, 255, 255); line-height: 1.5; "> Let's consider an example, assume stars randomly distributed around us with density n, what is probability that the nearest star is at distance $R$ ? Let's first consider the total number of stars within a differential volume, </p>
							  <div class="math">$$       N=ndV=4\pi R^2 n dR. $$</div>
							  <p style="color: rgb(255, 255, 255); line-height: 1.5; "> The probability of have no stars within $R$ is given by a Poisson process,  </p>
							  <div class="math">$$         p(R>r,\text{{0 star}}|n) \sim  \frac{(ndV)^0}{0!} e^{-n \frac{4\pi}{3} R^3 }, $$</div>
							  <p style="color: rgb(255, 255, 255); line-height: 1.5; "> and it is worth noting that it only tells a proportionality without normalization. Similarly, the probability of having one star within the radius is proportional to the following, </p>
							  <div class="math">$$         p(r \approx R,\text{{1 star}}|n) \sim  
								\frac{(ndV)^1}{1!}e^{-dV}=\frac{(n4\pi R^2 dR)^1}{1!} e^{-n4\pi R^2 dR }. $$</div>
								<p style="color: rgb(255, 255, 255); line-height: 1.5; "> Therefore, the joint probability has the product form, </p>
								<div class="math">$$      \begin{aligned}
									& p(R>r,\text{{0 star}}, r \approx R,\text{{1 star}}|n) =     p(R>r,\text{{0 star}}|n) p(r \approx R,\text{{1 star}}|n) \\ &\sim   n4\pi R^2 dR   e^{-n4\pi R^2 dR }  e^{-n \frac{4\pi}{3} R^3 } \approx   n4\pi R^2 dR   e^{-n \frac{4\pi}{3} R^3 }.
								\end{aligned} $$</div>
 
								
							</article>

		<article class="footer_column">
 

				<h3 style="color: aliceblue", class="header-h1"> Frequentist Inference </h3>
				<p style="color: rgb(255, 255, 255); line-height: 1.5; "> The frequentist uses the concept of frequency or repeated sampling. 
					It focuses on estimating statistical parameters, 
					e.g. means and variances and making statistical inferences based on the given data.
					 In frequentist statistics, parameters governing the underlying distribution 
					 are treated as fixed, unknown values, and the goal is to estimate these parameters 
					 using point estimates (e.g., maximum likelihood) or confidence intervals. 
					 For example, in the Stern-Gerlach experiment, the underlining distribution is 
					 \textit{assumed} to be the binomial distribution and the controlling parameter is $p=p_0$. 
					 The frequentist approach does not incorporate prior beliefs on the parameter $p$ 
					 (it is given and fixed), and it relies solely on the data at hand.    </p>
	
					 <p style="color: rgb(255, 255, 255); line-height: 1.5; ">   Hypothesis testing is a key component of frequentist statistics: 
						we start with two competing hypotheses, null and alternative hypothesis.
						 The former is a declaration of no difference, whereas the later stands 
						 in direct opposition to the null, suggesting a significant difference as an alternative. 
						 As an example, in the Stern-Gerlach experiment, the frequentist would first propose 
						 a binomial parameter $p_0=0.5$ with a null hypothesis: 
						 ``the probability of electron with spin up configuration is equal to 0.5''. 
						 Using statistical tests and analysis, such as confidence intervals, 
						 the frequentist approach would then assess whether the observed outcomes 
						 significantly deviate from the null hypothesis's expected probability of 0.5. 
						 The alternative hypothesis usually rejects the null, e.g. 
						 ``the probability of electron with spin up configuration is \textit{not} equal to 0.5''.   </p>
					
						 <h3 style="color: aliceblue", class="header-h1"> Bayesian Inference </h3>
						 <p style="color: rgb(255, 255, 255); line-height: 1.5; "> The Bayesian approach is based on Bayes' theorem, </p>
						 <div class="math">$$       P(\mathrm{ Parameter|Data} )=\frac{P(\mathrm {Data|Parameter})P(\mathrm {Parameter})}{P(\rm Data)},   $$</div>
						 <p style="color: rgb(255, 255, 255); line-height: 1.5; "> where ``Data'' represents the observed data, 
							and ``Parameter'' represents the variables that govern the underlying probability distribution function from which the observed data is drawn.  </p>
							<p style="color: rgb(255, 255, 255); line-height: 1.5; ">$  P(\mathrm{ Parameter|Data} )$  is referred to as the <strong>posterior</strong> probability, 
								representing the conditional probability of governing parameters given a set of observed data points.
								 $P(\mathrm {Data|Parameter})$ is the <strong>likelihood</strong> function that captures the probability 
								 of obtaining the observed data with the specified set of controlling parameters.
								  $P(\text{Parameter})$ is referred to as a <strong>prior</strong> distribution, 
								  which contains the assumptions regarding the probability distribution of the controlling parameters. 
								  $P(\rm Data)$ is known as the <strong>evidence</strong>, representing the overall probability of observing a particular dataset. 
								  Since we are mainly interested in the relative probability of various controlling parameters. 
								  Therefore, we absorb it into the normalization coefficient.  </p>
								  <p style="color: rgb(255, 255, 255); line-height: 1.5; ">Bayesian inference treats both observed data and parameters as random variables with probability distributions, 
									in contrast to the frequentist approach. Bayesian inference typically begins with prior 
									beliefs or knowledge about the controlling parameters.
									 For instance, in the Stern-Gerlach experiment, one might assume a prior probability 
									 distribution for the parameter $p$, such as a uniform distribution. 
									 The likelihood function quantifies the probability of observing the data with a 
									 specific set of controlling parameters and often relies on certain assumptions. 
									 For example, it might be described by a binomial distribution with the controlling 
									 parameter $p$ in the Stern-Gerlach experiment. The posterior probability represents
									 the updated beliefs about controlling parameters after integrating the observed data.
									  It can then be employed as the new prior for subsequent observations, enabling the 
									  continuous refinement of beliefs in the presence of additional data. </p>
									  
									  <h3 style="color: aliceblue", class="header-h1"> Bayesian v.s. Frequentist Inference </h3>

									  <p style="color: rgb(255, 255, 255); line-height: 1.5; ">Bayesian inference incorporate subjective prior beliefs and provide posterior probability 
										distributions of the controlling parameters, while frequentist inference focus on objective measures based 
										solely on observed data. The choice between the two approaches often depends 
										on the specific problem and available data.
									  </p>
									  <h3 style="color: aliceblue", class="header-h1"> .</h3>
									  <h3 style="color: aliceblue", class="header-h1"> .</h3>

									  <h3 style="color: aliceblue", class="header-h1"> .</h3>
									  <h3 style="color: aliceblue", class="header-h1"> .</h3>
									  <h3 style="color: aliceblue", class="header-h1"> .</h3>
									  <h3 style="color: aliceblue", class="header-h1"> .</h3>

									</article>
			
 


					

							

	
								<footer>
								</div>	
									<div class="copyright">&copy;2023 Zihang Wang@UCSB</div>
							</footer>
							
						</div>
	
  <!-- <section class="banner">
    <h2 class="parallax">Testing </h2>
    <p class="parallax_description">Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam</p>
	  <div class="button">subscribe</div>
  </section>		
		 -->
  <!-- Footer Section -->

  <!-- Copyrights Section -->
  

<!-- Main Container Ends -->
		
		
		
		
		
		
		
		
		
		
		
		
</body>
</html>